# -*- coding: utf-8 -*-
"""Classification of emotions using Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjjIzkXx3EE2_cqvC7xCHb2zg8_OQQ6O

# Classification of emotions

## Importing the libraries
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab.patches import cv2_imshow
import zipfile
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization

"""## Loading the images"""

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/.shortcut-targets-by-id/1fnn6e8q7Ykp7nqOPWGeSiijlNTt_kkPL/Computer Vision Masterclass/Datasets/fer_images.zip'
zip_object = zipfile.ZipFile(file=path, mode='r')
zip_object.extractall('./')
zip_object.close()

tf.keras.preprocessing.image.load_img('/content/fer2013/train/Angry/1003.jpg')

image = tf.keras.preprocessing.image.load_img('/content/fer2013/train/Happy/1.jpg')
image

"""### Train and test set"""

training_generator = ImageDataGenerator(rescale=1./255,
                                        rotation_range=7,
                                        horizontal_flip=True,
                                        zoom_range=0.2)
train_dataset = training_generator.flow_from_directory('/content/fer2013/train',
                                                        target_size = (48, 48),
                                                        batch_size = 16,
                                                        class_mode = 'categorical',
                                                        shuffle = True)

train_dataset.classes

np.unique(train_dataset.classes, return_counts=True)

train_dataset.class_indices

sns.countplot(x = train_dataset.classes);

test_generator = ImageDataGenerator(rescale=1./255)
test_dataset = test_generator.flow_from_directory('/content/fer2013/validation',
                                                  target_size = (48, 48),
                                                  batch_size = 1,
                                                  class_mode = 'categorical',
                                                  shuffle = False)

"""## Building and training the convolutional neural network"""

2*2*2*32

# Based on: https://github.com/rajeevratan84/DeepLearningCV/blob/master/18.2%20Building%20an%20Emotion%20Detector%20with%20LittleVGG.ipynb
# Padding: https://www.pico.net/kb/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-tensorflow
# BatchNormalization: https://keras.io/api/layers/normalization_layers/batch_normalization/
# Dropout: https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf
num_detectors = 32
num_classes = 7
width, height = 48, 48
epochs = 70

network = Sequential()

network.add(Conv2D(num_detectors, (3,3), activation='relu', padding = 'same', input_shape = (width, height, 3)))
network.add(BatchNormalization())
network.add(Conv2D(num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Conv2D(2*2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(Conv2D(2*2*2*num_detectors, (3,3), activation='relu', padding = 'same'))
network.add(BatchNormalization())
network.add(MaxPooling2D(pool_size=(2,2)))
network.add(Dropout(0.2))

network.add(Flatten())

network.add(Dense(2 * num_detectors, activation='relu'))
network.add(BatchNormalization())
network.add(Dropout(0.2))

network.add(Dense(2 * num_detectors, activation='relu'))
network.add(BatchNormalization())
network.add(Dropout(0.2))

network.add(Dense(num_classes, activation='softmax'))
print(network.summary())

network.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

network.fit(train_dataset, epochs=epochs)

"""## Saving and loading the model"""

model_json = network.to_json()
with open('network_emotions.json','w') as json_file:
  json_file.write(model_json)

from keras.models import save_model
network_saved = save_model(network, '/content/weights_emotions.hdf5')

with open('/content/drive/MyDrive/Cursos - recursos/Computer Vision Masterclass/Weights/network_emotions.json', 'r') as json_file:
  json_saved_model = json_file.read()
json_saved_model

network_loaded = tf.keras.models.model_from_json(json_saved_model)
network_loaded.load_weights('/content/drive/MyDrive/Cursos - recursos/Computer Vision Masterclass/Weights/weights_emotions.hdf5')
network_loaded.compile(loss = 'categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])

network_loaded.summary()

"""## Evaluating the neural network"""

network_loaded.evaluate(test_dataset)

predictions = network_loaded.predict(test_dataset)
predictions

predictions = np.argmax(predictions, axis = 1)
predictions

test_dataset.classes

from sklearn.metrics import accuracy_score
accuracy_score(test_dataset.classes, predictions)

test_dataset.class_indices

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_dataset.classes, predictions)
cm

sns.heatmap(cm, annot=True);

from sklearn.metrics import classification_report
print(classification_report(test_dataset.classes, predictions))

"""## Classifying one single image"""

image = cv2.imread('/content/drive/MyDrive/Cursos - recursos/Computer Vision Masterclass/Images/gabriel.png')
cv2_imshow(image)

image.shape

face_detector = cv2.CascadeClassifier('/content/drive/MyDrive/Cursos - recursos/Computer Vision Masterclass/Cascades/haarcascade_frontalface_default.xml')

original_image = image.copy()
faces = face_detector.detectMultiScale(original_image)

faces

roi = image[40:40 + 128, 162:162 + 128]
cv2_imshow(roi)

roi.shape

roi = cv2.resize(roi, (48, 48))
cv2_imshow(roi)

roi.shape

roi

roi = roi / 255
roi

roi.shape

roi = np.expand_dims(roi, axis = 0)
roi.shape

probs = network_loaded.predict(roi)
probs

result = np.argmax(probs)
result

test_dataset.class_indices

"""## Classifying multiple images"""

image = cv2.imread('/content/drive/MyDrive/Cursos - recursos/Computer Vision Masterclass/Images/faces_emotions.png')
cv2_imshow(image)

faces = face_detector.detectMultiScale(image)
faces

test_dataset.class_indices.keys()

emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

for (x, y, w, h) in faces:
  cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 1)
  roi = image[y:y + h, x:x + w]
  #cv2_imshow(roi)
  roi = cv2.resize(roi, (48, 48))
  #cv2_imshow(roi)
  roi = roi / 255
  roi = np.expand_dims(roi, axis = 0)
  #print(roi.shape)
  prediction = network_loaded.predict(roi)
  #print(prediction)
  cv2.putText(image, emotions[np.argmax(prediction)], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)
cv2_imshow(image)

"""## Classifying emotions in videos"""

cap = cv2.VideoCapture('/content/drive/MyDrive/Cursos - recursos/Computer Vision Masterclass/Videos/emotion_test01.mp4')
connected, video = cap.read()
print(connected, video.shape)

# fourcc.org
save_path = '/content/drive/MyDrive/Cursos - recursos/Computer Vision Masterclass/Videos/emotion_test01_result.avi'
fourcc = cv2.VideoWriter_fourcc(*'XVID')
fps = 24
output_video = cv2.VideoWriter(save_path, fourcc, fps, (video.shape[1], video.shape[0]))

while (cv2.waitKey(1) < 0):
  connected, frame = cap.read()
  if not connected:
    break
  faces = face_detector.detectMultiScale(frame, scaleFactor=1.2, minNeighbors=5, minSize=(30,30))
  if len(faces) > 0:
    for (x, y, w, h) in faces:
      frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
      roi = frame[y:y + h, x:x + w]
      roi = cv2.resize(roi, (48, 48))
      roi = roi / 255
      roi = np.expand_dims(roi, axis = 0)
      prediction = network_loaded.predict(roi)

      if prediction is not None:
        result = np.argmax(prediction)
        cv2.putText(frame, emotions[result], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)

  cv2_imshow(frame)
  output_video.write(frame)

print('End')
output_video.release()
cv2.destroyAllWindows()