# -*- coding: utf-8 -*-
"""Style Transfer in Computer Vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZEgvJpw_XbYHrSDXQ6eExyr7K-EBjQFc

# Computer Vision Masterclass - Style Transfer

- Based on https://www.tensorflow.org/tutorials/generative/style_transfer?hl=en
- Style transfer paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf

## Importing the libraries
"""

import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
tf.__version__

"""## Loading the pre-trained convolutional neural network (VGG19)

"""

network = tf.keras.applications.VGG19(include_top=False, weights='imagenet')

network.summary()

len(network.layers)

"""## Loading and pre-processing the images

### Content image
"""

from google.colab import drive
drive.mount('/content/drive')

content_image = tf.keras.preprocessing.image.load_img('/content/drive/.shortcut-targets-by-id/1fnn6e8q7Ykp7nqOPWGeSiijlNTt_kkPL/Computer Vision Masterclass/Images/chaves.jpeg')

plt.imshow(content_image);

type(content_image)

content_image = tf.keras.preprocessing.image.img_to_array(content_image)

type(content_image), content_image.shape, content_image.min(), content_image.max()

content_image = content_image / 255

content_image.min(), content_image.max()

content_image

content_image = content_image[tf.newaxis, :]

content_image.shape

"""### Style image"""

style_image = tf.keras.preprocessing.image.load_img('/content/drive/.shortcut-targets-by-id/1fnn6e8q7Ykp7nqOPWGeSiijlNTt_kkPL/Computer Vision Masterclass/Images/tarsila_amaral.jpg')

plt.imshow(style_image);

style_image = tf.keras.preprocessing.image.img_to_array(style_image)
style_image = style_image / 255
style_image = style_image[tf.newaxis, :]
style_image.shape

style_image = tf.keras.preprocessing.image.load_img('/content/drive/.shortcut-targets-by-id/1fnn6e8q7Ykp7nqOPWGeSiijlNTt_kkPL/Computer Vision Masterclass/Images/vangogh.jpg')

plt.imshow(style_image);

style_image = tf.keras.preprocessing.image.img_to_array(style_image)
style_image = style_image / 255
style_image = style_image[tf.newaxis, :]
style_image.shape

"""## Building the neural network"""

content_layers = ['block4_conv2']
style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)
print(num_content_layers, num_style_layers)

[network.get_layer(name).output for name in style_layers]

network.input

def vgg_layers(layer_names):
  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  vgg.trainable = False

  outputs = [vgg.get_layer(name).output for name in layer_names]
  network = tf.keras.Model(inputs = [vgg.input], outputs = outputs)

  return network

style_extractor = vgg_layers(style_layers)

style_extractor.summary()

style_extractor.outputs

style_outputs = style_extractor(style_image)

len(style_outputs)

style_outputs[1]

style_outputs[0].shape, style_outputs[1].shape, style_outputs[2].shape, style_outputs[3].shape, style_outputs[4].shape

# https://www.tensorflow.org/api_docs/python/tf/einsum

# Loss between the style and the content image (see original paper, section 2.2)
def gram_matrix(layer_activation):
  result = tf.linalg.einsum('bijc,bijd->bcd', layer_activation, layer_activation)
  input_shape = tf.shape(layer_activation)
  num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)

  return result / num_locations

style_outputs[0]

gram_matrix(style_outputs[0])

class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super().__init__()
    self.vgg = vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    inputs = inputs * 255.0
    # 0 - 1
    # -127.50 - 127.50
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs = outputs[:self.num_style_layers]
    content_outputs = outputs[self.num_style_layers:]

    style_outputs = [gram_matrix(style_output) for style_output in style_outputs]

    content_dict = {content_name: value for content_name, value in zip(self.content_layers, content_outputs)}
    style_dict = {style_name: value for style_name, value in zip(self.style_layers, style_outputs)}

    return {'content': content_dict, 'style': style_dict}

style_layers, content_layers

extractor = StyleContentModel(style_layers, content_layers)

results = extractor(content_image)

results

for key, value in results.items():
  print(key, value.keys())

style_targets = extractor(style_image)['style']
content_targets = extractor(content_image)['content']

len(style_targets), len(content_targets)

new_image = tf.Variable(content_image)

content_weight = 1
style_weight = 100

optimizer = tf.optimizers.Adam(learning_rate=0.02)

"""## Training"""

expected_output = np.array([10000, 15000])
predictions = np.array([8000, 15900])

np.sum(abs(expected_output - predictions)) / 2

from sklearn.metrics import mean_absolute_error, mean_squared_error

mean_absolute_error(expected_output, predictions)

mean_squared_error(expected_output, predictions)

2 ** 3, 2 * 2 * 2

10 ** 2, 10 * 10

((expected_output - predictions) ** 2).mean()

plt.imshow(tf.squeeze(content_image, axis = 0));

epochs = 3000
print_every = 500

for epoch in range(epochs):
  with tf.GradientTape() as tape:
    outputs = extractor(new_image)

    content_outputs = outputs['content']
    style_outputs = outputs['style']

    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2) for name in content_outputs.keys()])
    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2) for name in style_outputs.keys()])

    total_loss = content_loss * content_weight / num_content_layers + style_loss * style_weight / num_style_layers

  gradient = tape.gradient(total_loss, new_image)
  optimizer.apply_gradients([(gradient, new_image)])

  new_image.assign(tf.clip_by_value(new_image, 0.0, 1.0))

  if (epoch + 1) % print_every == 0:
    print('Epoch {} | content loss: {} | style loss: {} | total loss {}'.format(epoch + 1, content_loss, style_loss, total_loss))
    plt.imshow(tf.squeeze(new_image, axis = 0))
    plt.show()

"""## Visualizing the results"""

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,8))
ax1.imshow(tf.squeeze(content_image, axis = 0))
ax1.set_title('Content image')
ax2.imshow(tf.squeeze(new_image, axis = 0))
ax2.set_title('New image')
ax3.imshow(tf.squeeze(style_image, axis = 0))
ax3.set_title('Style image')
plt.axis('off');

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 8))
ax1.imshow(tf.squeeze(content_image, axis = 0))
ax1.set_title('Content/original image')
ax2.imshow(tf.squeeze(new_image, axis = 0))
ax2.set_title('New image')
ax3.imshow(tf.squeeze(style_image, axis = 0))
ax3.set_title('Style imagem')
plt.axis('off');